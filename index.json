[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a second year PhD student at Magdalen College at the Univeristy of Oxford, in the Autonomous Intelligent Machines and Systems CDT (AIMS) programme. I\u0026rsquo;m supervised by Tom Rainforth, Eric Nalisnick, and Yee Whye Teh.\nI have broad interests in Bayesian and probabilistic machine learning, including Bayesian deep learning. I\u0026rsquo;m also excited by deep unsupervised learning and deep generative models. I hope to develop techniques that allow these methods to be more readily applied to real world problems.\nOf late, I\u0026rsquo;ve been focusing on COVID-19 nonpharmaceutical intervention research. In short, we hope to understand the individual effectiveness of different government interventions at controlling the spread of COVID-19?\u0026rdquo;\nGot questions about applying for ML PhDs at Oxford? Feel free to reach out, either by email or on Twitter :)\n","date":1572134400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1572134400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mrinanksharma.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a second year PhD student at Magdalen College at the Univeristy of Oxford, in the Autonomous Intelligent Machines and Systems CDT (AIMS) programme. I\u0026rsquo;m supervised by Tom Rainforth, Eric Nalisnick, and Yee Whye Teh.\nI have broad interests in Bayesian and probabilistic machine learning, including Bayesian deep learning. I\u0026rsquo;m also excited by deep unsupervised learning and deep generative models. I hope to develop techniques that allow these methods to be more readily applied to real world problems.","tags":null,"title":"Mrinank Sharma","type":"authors"},{"authors":["**Mrinank** **Sharma***","Sören Mindermann*","Jan Markus Brauner*","Gavin Leech","Anna B. Stephenson","Tomáš Gavenčiak","Jan Kulveit","Yee Whye Teh","Leonid Chindelevitch","Yarin Gal"],"categories":[],"content":"","date":1606176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606176000,"objectID":"7d4c3597db29e5f68ca3285ee31fd7a8","permalink":"https://mrinanksharma.github.io/publication/covidone/","publishdate":"2020-11-24T00:00:00Z","relpermalink":"/publication/covidone/","section":"publication","summary":"To what extent are effectiveness estimates of nonpharmaceutical interventions (NPIs) against COVID-19 influenced by the assumptions our models make? To answer this question, we investigate 2 state-of-the-art NPI effectiveness models and propose 6 variants that make different structural assumptions. In particular, we investigate how well NPI effectiveness estimates generalise to unseen countries, and their sensitivity to unobserved factors. Models which account for noise in disease transmission compare favourably. We further evaluate how robust estimates are to different choices of epidemiological parameters and data. Focusing on models that assume transmission noise, we find that previously published results are robust across these choices and across different models. Finally, we mathematically ground the interpretation of NPI effectiveness estimates when certain common assumptions do not hold.","tags":["COVID19"],"title":"How Robust are the Estimated Effects of Nonpharmaceutical Interventions against COVID-19?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"d6e9c158f2dd63a0dff4939f6d41cb56","permalink":"https://mrinanksharma.github.io/news/covid_neurips/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/news/covid_neurips/","section":"news","summary":"**How Robust are the Estimated Effects of Nonpharmaceutical Interventions against COVID-19?** has been accepted as a **spotlight talk** at NeurIPS 2020!","tags":null,"title":"NeurIPS 2020 Spotlight Accept","type":"news"},{"authors":["**Mrinank** **Sharma***","Michael Hutchinson*","Siddharth Swaroop","Antti Honkela","Richard E. Turner"],"categories":[],"content":"","date":1574553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574553600,"objectID":"2d13f8a00e94a56fc947390c8f6efb84","permalink":"https://mrinanksharma.github.io/publication/primlone/","publishdate":"2019-11-24T00:00:00Z","relpermalink":"/publication/primlone/","section":"publication","summary":"In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on. Furthermore, each client's data, computational resources and communication constraints may be very different. This setting is known as federated learning, in which privacy is a key concern. Differential privacy is commonly used to provide mathematical privacy guarantees. This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning. We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting. We modify the client-side optimisation of PVI to provide an (ϵ, δ)-DP guarantee. We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.","tags":["Privacy"],"title":"Differentially Private Federated Variational Inference","type":"publication"},{"authors":["Mrinank Sharma"],"categories":null,"content":" I attended EA Global for the first time in October, and I absolutely loved the experience. I thought it would be useful to go over all the notes that I made, mostly for myself, but also on the off-chance that it would be helpful for other people.\nI\u0026rsquo;ve summarised some of the notes that I\u0026rsquo;ve made on general topics below. Please note that there may be errors, and that I may have misrepresented people\u0026rsquo;s views though this is certainly not my intention!\nMaking the Most of EA Global The primary advice that I read before going was to maximise the time spent in one-on-one meetings and workshops as opposed to talks, most of which are later uploaded online. I only ended up filling in my Whova (conference application) profile fairly late, but would strongly recommend doing this (early!), as well as reaching out to people who share similar interests to you. I think the advice that I recieved was mostly spot on, and the most useful experiences that I had were certainly these one-on-one meetings.\nI\u0026rsquo;d also like to echo the advice to write down, or at least consider, your goals for the conference.\nAdditionally, bring a notepad and make notes! You\u0026rsquo;ll inevitably forget something really useful.\nThis EA forum post by Risto Uuk is great.\nAI Policy \u0026amp; Governmence Broadly, we can divide the roles in this field into researchers and those implementing policies (either within industry, or within government). If you are interested in AI policy, it is best to focus primarily on fit within the role rather than shoe-horn yourself into a role which you may think is \u0026ldquo;more important\u0026rdquo;.\nYou are a good fit for a researcher if:\n You are happy and excited by applying concentrated effort on one particular idea. You have strong, internal motivation and are self driven. You are comfortable working without much supervision; it seems that there is a dearth of supervision at the moment. You are happy with being far from the decision making process in time.  You are a good fit for a role in government or industry if:\n You are a natural generalist, and enjoy working on many problems at once. You are extroverted, and have strong social skills. You are likeable, trustworthy and good at small talk. You are patient. It can be frustrating when people do not implement precisely what you are suggesting.  This is not to say that it isn\u0026rsquo;t useful being a researcher who has excellent social skills!\nConsidering long-termism, you ought to try and figure out needs to happen for a positive outcome post-AGI. The decisions made today will affect the future landscape, but when attempting to convince other people, beware that the long-termism standpoint may alienate them.\nDo not underestimate the importance of institutional work; trying to improve institution capacity and establishing norms can be useful.\nDo you need technical expertise in AI? The advice that I heard was that it is mostly not necessary; unless you are already doing a PhD in ML/AI, it is probably not worth pursuing one. However, whilst most questions that you will be trying to answer will not benefit from this knowledge, there are a unique set of questions which you will be better equiped to answer, such as understanding the strategic important of new developments.\nA good target of technical expertise would be to be able to make sense of the Import AI Newsletter.\nSo if you shouldn\u0026rsquo;t do an ML PhD, what should you do? The advice seems to be that a degree in International Relations would be very useful.\nIndustry vs Government It seems that government work is more neglected compared to working in an industry lab. This experience is still be useful, but perhaps more of a middle step? It is better to learn skills and advance your reputation in industry, and people in such labs end up advising government anyway. It tends to be slower to build up credibility in government positions.\nAI Research Applying for Internships at OpenAI Many internships at OpenAI are organised on an adhoc basis; if there is somebody you specifically want to work with, it\u0026rsquo;s best to send them an email with a few ideas, suggesting a collaboration.\nLabs vs Academia There is less pressure to publish at OpenAI compared to more traditional academia, and there is a significantly higher focus on impact i.e., the choice of project depends on the OpenAI mission. This is typically not the case in academia.\nAt OpenAI, the research leads suggest project proposals and roadmaps, which are then iterated upon.\nOpenAI seems to focus on current techniques more and neuroscience less than Deepmind (I\u0026rsquo;m not entirely sure how accurate this is).\nChoosing a PhD Topic If you are unable to work in safety directly, bear in mind that the transition from normal ML research to safety research seems to be doable and commonly done. You could then choose your topic by considering the following factors:\n Prestige; working on a topic which is more likely to be read by others may lead to gaining a higher potential for impact on the future. Personal interest. Commerical incentives; consider what problems are likely to be neglected by industry (i.e., those problems for which there is no commerical incentive to do that research) but are still important. Immediate impact; working on a topic which has direct applications, for example in healthcare scenarious, could be beneficial. Skill development; which topics give you the opportunity to develop your skills in a number of different, useful areas. Neglectedness; consider what the marginal impact is of one additional researcher in XYZ.  Keeping Up with Papers There are a vast numbers of papers to read, especially in AI! Keeping up with research is hard, but a simple way of prioritising is to ask senior people about which papers to read. Slowly, you will develop intution about which papers to prioritise.\nAlso follow the Import AI Newsletter, as well as the AI Alignment newsletter.\nWhat\u0026rsquo;s Stopping Advanced Applications of AI? In many cases, there are cultural issues (within an industry) about the application of algorithms to make crucial decisions. Whilst interpretability of systems would increase the buy in, there are also key issues with the quality of data, and the infrastructure to collect high quality data.\nIt is worth nothing that the barriers here seem to not be technical, so it is unclear how much of an impact technical research would have here.\nClosing Comments I absolutely loved attending EA Global 2019, and one of the most beneficial things of going there was starting to build up a network of people who share similar interests. I learnt a huge deal from other people, and strongly recommend going if you are on the fence!\nI\u0026rsquo;ve you\u0026rsquo;ve spotted any errors in this post, please do contact me and I\u0026rsquo;ll do my best to respond and to fix them.\n","date":1572134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572134400,"objectID":"f5751c5229a534a81fb911daa326d22d","permalink":"https://mrinanksharma.github.io/post/eag_reflect/","publishdate":"2019-10-27T00:00:00Z","relpermalink":"/post/eag_reflect/","section":"post","summary":"Summarised notes that I made at the conference.","tags":["Effective Altruism","AI Safety","AI Governance"],"title":"Reflections on EA Global 2019","type":"post"},{"authors":null,"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"e21332125f8b2f8e079f9917bf41ef26","permalink":"https://mrinanksharma.github.io/news/bayesian_logistic_regression/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/news/bayesian_logistic_regression/","section":"news","summary":"*Differentially Private Federated Variational Inference* has been accepted at the NeurIPS Privacy in Machine Learning Workshop.","tags":null,"title":"Paper Accepted at NeurIPS Privacy in Machine Learning Workshop","type":"news"},{"authors":["Mrinank Sharma"],"categories":[],"content":"Machine learning is often applied in contexts in which:\n The data is distributed across many different client devices. Think mobile phones, autonomous vehicles, Internet of Things devices \u0026hellip; The data contains to private, sensitive information about individuals, for example, healthcare data. We want to train a machine learning model to help us to make decisions i.e. we want to be Bayesian and model uncertainty.  Examples of where all three of the above hold include smartphone keyboard text prediction, and healthcare drug sensitivity analysis (i.e., which drug should be given to a particular patient).\nThe task of learning an accurate Bayesian model on distributed data is already somewhat challenging, but when individuals contribute sensitive data, there are additional concerns. Will the model predictions inadvertantly leak information about the individuals used to train the model? In fact, neural networks have been show to exhibit the property of memorising training examples 1. Given this, how can we convince new users to contribute their data? Note that we want to ensure privacy in the sense that when the full trained model is released to the public, the information about individual users is still protected.\nDifferential Privacy (DP) 2 is the current gold standard for mathematically quantifying the level of privacy offered by a randomised algorithm. Note that there is a requirement for the algorithm to be randomised, and this can be interpreted as giving each individual some plausibility deniability.\nThis project built upon Partitioned Variational Inference (PVI) 3, a recently developed framework which supports federated (distributed) variational inference, a form of approximate inference, adapting the algorithm to provide a differential privacy guarantee.\nRelevant Publications:\n Differentially Private Federated Variational Inference   Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song, Úlfar Erlingsson, and Dawn Song. The Secret Sharer: Measuring Unintended Neural Network Memorization \u0026amp; Extracting Secrets. 2018. arVix ^ Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–487, 2013. ISSN 15513068. doi:10.1561 / 0400000042. ^ Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, and Richard E. Turner. Partitioned Variational Inference: A unified framework encompassing federated and continual learning. November 2018. arXiv ^   ","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"84236f6ace32bc0eb9a2ea7b0359f306","permalink":"https://mrinanksharma.github.io/project/privacy_project/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/project/privacy_project/","section":"project","summary":"Learning Private, Bayesian Machine Learning Models in the Federating Learning Context","tags":["Privacy"],"title":"Differential Privacy and Approximate Bayesian Inference","type":"project"}]